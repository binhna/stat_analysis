{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T15:50:19.244888Z",
     "start_time": "2018-07-02T15:50:18.041130Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score, auc\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T15:50:20.935596Z",
     "start_time": "2018-07-02T15:50:20.570929Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset and test_dataset are both SimpleDataset objects,\n",
    "# which is a wrapper for lists and arrays.\n",
    "train_dataset, test_dataset = [nlp.data.IMDB(segment=segment)\n",
    "                               for segment in ('train', 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T15:51:23.555776Z",
     "start_time": "2018-07-02T15:50:23.259263Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freq = collections.Counter()\n",
    "max_len = 0\n",
    "num_rec = 0\n",
    "\n",
    "for d in train_dataset:\n",
    "    sentence, label = d[0], d[1]\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    if len(words) > max_len:\n",
    "        max_len = len(words)\n",
    "    for word in words:\n",
    "        word_freq[word] += 1\n",
    "    num_rec += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-02T15:51:23.589154Z",
     "start_time": "2018-07-02T15:51:23.557682Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "# most_common output -> list\n",
    "word2idx = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(MAX_FEATURES - 2))}\n",
    "word2idx ['PAD'] = 0\n",
    "word2idx['UNK'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word= {i:v for v, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "origin_txt = []\n",
    "\n",
    "\n",
    "for d in train_dataset:\n",
    "    _sentence, _label = d[0].decode('utf8').strip().split('\\t'), d[1]\n",
    "    origin_txt.append(_sentence)\n",
    "    y.append(int(_label))\n",
    "    words = nltk.word_tokenize(_sentence.lower())\n",
    "    for word in words:\n",
    "        if word in word2idx.keys():\n",
    "            _seq.append(word2idx[word])\n",
    "        else:\n",
    "            _seq.append(word2idx['UNK'])\n",
    "            \n",
    "    if len(_seq) < MAX_SENTENCE_LENGTH:\n",
    "        _seq.extend([0] * ((MAX_SENTENCE_LENGTH) - len(_seq)))\n",
    "    else:\n",
    "        _seq = _seq[:MAX_SENTENCE_LENGTH]\n",
    "        x.append(_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, vocab_size):\n",
    "    res = np.zeros(shape = (vocab_size))\n",
    "    res[x] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "* Input data shape::$ (batch \\times word \\times vocab )$ \n",
    "* Split data into two pieces: training & validation\n",
    "* Create data iterator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.array([np.array([one_hot(word, MAX_FEATURES) for word in example]) for example in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx = np.random.choice(range(x_1.shape[0]), int(x_1.shape[0] * .8))\n",
    "va_idx = [x for x in range(x_1.shape[0]) if x not in tr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x = x_1[tr_idx, :]\n",
    "tr_y = [y[i] for i in tr_idx]\n",
    "va_x = x_1[va_idx, :]\n",
    "va_y = [y[i] for i in va_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "batch_size = 16\n",
    "train_data = mx.io.NDArrayIter(data=[tr_x, tr_y], batch_size=batch_size, shuffle = False)\n",
    "valid_data = mx.io.NDArrayIter(data=[va_x, va_y], batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Self-Attention Network\n",
    "\n",
    "* We need hidden representation at first. In the original paper of SA, authors used bidirectional LSTM to end up with hidden representation, but it doesn't have to be LSTM. Any deep learning technique can be applied.\n",
    "\n",
    "* In this notebook, we applied relation network . As is said before, relation network take into account all pair-wise relationships between tokens in the sentence via the following equation\n",
    "  \n",
    "  $$ f(x_i, x_j ) =W \\phi(U_{left} e_i + U_{right} e_j)$$\n",
    "\n",
    "* As the result of the above computation, we get the following hidden representation of the sentence.\n",
    "\n",
    "$$ H = [H_1, \\ldots, H_n], \\textrm{ where  }H_i = [f(x_i, x_1), \\ldots , f(x_i, x_n]$$\n",
    "\n",
    "when there are $n$ tokens in the sentence.\n",
    "\n",
    "* Self- Attention matrix is defined as follows\n",
    "\n",
    "$$ A = softmax(W_2 tanh (W_1 H^T))$$\n",
    "\n",
    "* $W_1$ is of size $d \\times n$, where $d$ is an hyper parameter that should be determined by user.\n",
    "\n",
    "* $W_2$ is another linear transformation, which results in actual attention vector. The size of $A$ is $r\\times n$, where $r$ is the dimension of attention vector.\n",
    "\n",
    "* The final sentence represention is given by\n",
    "\n",
    "$$ M = A H$$\n",
    "\n",
    "* Here, we note that a single sentence is summarized as a set of vectors, not a single vector. This makes the representation more clear so that it may play a role in boosting its performance comparing to single vector summary such as the last step of hidden layer in RNN or average of CBoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.ndarray.linalg import gemm2\n",
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SA_Classifier(nn.HybridBlock):\n",
    "    def __init__(self, _SENTENCE_LENGTH, _VOCABULARY, _ATTENTION_DIM, _D_A, _BATCH_SIZE, **kwargs):\n",
    "        super(SA_Classifier, self).__init__(**kwargs)\n",
    "        self._SENTENCE_LENGTH = _SENTENCE_LENGTH\n",
    "        self._ATTENTION_DIM = _ATTENTION_DIM\n",
    "        self._VOCABULARY = _VOCABULARY\n",
    "        self._D_A = _D_A\n",
    "        self._BATCH_SIZE = _BATCH_SIZE\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.g_fc1 = nn.Dense(256,activation='relu')\n",
    "            self.g_fc2 = nn.Dense(128,activation='relu')\n",
    "            self.g_fc3 = nn.Dense(64,activation='relu')\n",
    "            self.g_fc4 = nn.Dense(1,activation='relu')\n",
    "\n",
    "            self.W1 = nd.random.normal(loc = 0, scale = 1, shape = (self._D_A, self._SENTENCE_LENGTH), ctx = context)\n",
    "            self.W2 = nd.random.normal(loc = 0, scale = 1, shape = (self._ATTENTION_DIM, self._D_A), ctx = context)\n",
    "            \n",
    "            self.fc1 = nn.Dense(128, activation = 'relu') # 256 * 128\n",
    "            self.fc2 = nn.Dense(2) # 128 * 2\n",
    "            # 1253632 param : 약 20MB\n",
    "            \n",
    "    def hybrid_forward(self, F, x):\n",
    "        # (x_i, x_j)의 pair를 만들기\n",
    "        x_i = x.expand_dims(1) # B * 1* 40 * 2000* : 0.02GB\n",
    "        x_i = F.repeat(x_i,repeats= self._SENTENCE_LENGTH, axis=1) # B * 40 * 40 * 2000: 1.52GB\n",
    "        x_j = x.expand_dims(2) # B * 40 * 1 * 2000\n",
    "        x_j = F.repeat(x_j,repeats= self._SENTENCE_LENGTH, axis=2) # B * 40 * 40 * 2000: 1.52GB\n",
    "        x_full = F.concat(x_i,x_j,dim=3) # B * 40 * 40 * 4000: 3.04GB\n",
    "        # batch*sentence_length*sentence_length개의 batch를 가진 2*VOCABULARY input을 network에 feed\n",
    "        _x = x_full.reshape((-1, 2 * self._VOCABULARY))\n",
    "        _x = self.g_fc1(_x) # (B * 40 * 40) * 256: .1GB 추가메모리는 안먹나?\n",
    "        _x = self.g_fc2(_x) # (B * 40 * 40) * 128: .1GB (reuse)\n",
    "        _x = self.g_fc3(_x) # (B * 40 * 40) * 64: .1GB (reuse)\n",
    "        _x = self.g_fc4(_x) # (B * 40 * 1) * 256: .1GB (reuse)\n",
    "\n",
    "        # Reshape to get Hidden representataion\n",
    "        H = _x.reshape((-1, self._SENTENCE_LENGTH, self._SENTENCE_LENGTH))\n",
    "        W1_rep = self.W1.expand_dims(0)\n",
    "        W1_rep = F.repeat(W1_rep, repeats = self._BATCH_SIZE, axis = 0)\n",
    "        _tmp1 = gemm2(W1_rep, H)\n",
    "        _tmp1 = nd.tanh(_tmp1)\n",
    "        W2_rep = self.W2.expand_dims(0)\n",
    "        W2_rep = F.repeat(W2_rep, repeats = self._BATCH_SIZE, axis = 0)    \n",
    "        A = nd.softmax(gemm2(W2_rep, _tmp1), axis = 2)\n",
    "        M = gemm2(A, H)\n",
    "        \n",
    "        # 여기서부터는 classifier\n",
    "        _M = M.reshape((-1, self._ATTENTION_DIM * self._SENTENCE_LENGTH))\n",
    "        \n",
    "        clf = self.fc1(_M)\n",
    "        clf = self.fc2(clf)\n",
    "        return clf, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 40, 40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.uniform(size = (4, 40, 2000))\n",
    "z = nd.array(z, ctx = context)\n",
    "sa = SA_Classifier(40, 2000, 40, 40, 4)\n",
    "sa.collect_params().initialize(mx.init.Xavier(), ctx =context)\n",
    "sa(z)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SA_Classifier(MAX_SENTENCE_LENGTH, MAX_FEATURES, MAX_SENTENCE_LENGTH, MAX_SENTENCE_LENGTH, batch_size)\n",
    "sa.collect_params().initialize(mx.init.Xavier(), ctx = context)\n",
    "loss = gluon.loss.SoftmaxCELoss()\n",
    "trainer = gluon.Trainer(sa.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37968edb8041425e842cd97ad7ecafa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: tr_loss = 0.20457202664963428, tr_acc= 0.9184859154929578, va_loss = 0.13729756747171662, va_acc= 0.9470443349753694\n",
      "Epoch 1: tr_loss = 0.05097090207332946, tr_acc= 0.9836267605633803, va_loss = 0.13267330863051743, va_acc= 0.9618226600985221\n",
      "Epoch 2: tr_loss = 0.026243403792482817, tr_acc= 0.9926056338028169, va_loss = 0.1539830091348803, va_acc= 0.957820197044335\n",
      "Epoch 3: tr_loss = 0.015879086651590126, tr_acc= 0.9952464788732395, va_loss = 0.1775123529325058, va_acc= 0.9655172413793104\n",
      "Epoch 4: tr_loss = 0.013689804695411888, tr_acc= 0.9973591549295775, va_loss = 0.13496094134290118, va_acc= 0.9682881773399015\n",
      "Epoch 5: tr_loss = 0.00640793367704133, tr_acc= 0.9985915492957746, va_loss = 0.16537107877836485, va_acc= 0.9664408866995073\n",
      "Epoch 6: tr_loss = 0.006708829911573301, tr_acc= 0.9984154929577465, va_loss = 0.32971756679130876, va_acc= 0.9652093596059114\n",
      "Epoch 7: tr_loss = 0.015708154947701544, tr_acc= 0.9948943661971831, va_loss = 0.1504514035003536, va_acc= 0.9676724137931034\n",
      "Epoch 8: tr_loss = 0.005975391798333357, tr_acc= 0.9985915492957746, va_loss = 0.1886441679092217, va_acc= 0.9692118226600985\n",
      "Epoch 9: tr_loss = 0.0017287729329599176, tr_acc= 0.9994718309859155, va_loss = 0.21178208828991105, va_acc= 0.9658251231527094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 약 2GB 정도 필요\n",
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(train_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        with autograd.record():\n",
    "            _out, _A = sa(_dat)\n",
    "            _los = nd.sum(loss(_out, _label)) # 배치의 크기만큼의 loss가 나옴\n",
    "            _los.backward()\n",
    "        trainer.step(_dat.shape[0])\n",
    "        n_obs += _dat.shape[0]\n",
    "        _total_los += nd.sum(_los).asscalar()\n",
    "        # Epoch loss를 구하기 위해서 결과물을 계속 쌓음\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy()) # 두번째 컬럼의 확률이 예측 확률\n",
    "        label.extend(_label.asnumpy())\n",
    "    tr_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    tr_loss = _total_los/n_obs\n",
    "    \n",
    "    ### Evaluate training\n",
    "    valid_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    attention = []\n",
    "    for i, batch in enumerate(valid_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        _out, _A = sa(_dat)\n",
    "        _pred_score = nd.softmax(_out)\n",
    "        n_obs += _dat.shape[0]\n",
    "        _total_los += nd.sum(loss(_out, _label)).asscalar()\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy())\n",
    "        label.extend(_label.asnumpy())\n",
    "        attention.extend(_A)\n",
    "    va_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    va_loss = _total_los/n_obs\n",
    "    tqdm.write('Epoch {}: tr_loss = {}, tr_acc= {}, va_loss = {}, va_acc= {}'.format(epoch, tr_loss, tr_acc, va_loss, va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>pred_sa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to be here because I love Harry Potter,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watched mission impossible 3 wif stupid haha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anyway, thats why I love \" Brokeback Mountain.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dudeee i LOVED brokeback mountain!!!!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I either LOVE Brokeback Mountain or think it's...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It was awesome, I finished up my tests, and af...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Da Vinci Code sucks be...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finally feel up to making the long ass drive o...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I hate Harry Potter.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Then snuck into Brokeback Mountain, which is t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  pred_sa  label\n",
       "0  I want to be here because I love Harry Potter,...      1.0      1\n",
       "1    watched mission impossible 3 wif stupid haha...      0.0      0\n",
       "2     Anyway, thats why I love \" Brokeback Mountain.      1.0      1\n",
       "3              dudeee i LOVED brokeback mountain!!!!      1.0      1\n",
       "4  I either LOVE Brokeback Mountain or think it's...      1.0      1\n",
       "5  It was awesome, I finished up my tests, and af...      0.0      1\n",
       "6                          Da Vinci Code sucks be...      0.0      0\n",
       "7  Finally feel up to making the long ass drive o...      1.0      1\n",
       "8                               I hate Harry Potter.      0.0      0\n",
       "9  Then snuck into Brokeback Mountain, which is t...      0.0      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to specify batch_size explicitly becuase we need that in reshaping\n",
    "idx = np.random.choice(len(va_idx), batch_size)\n",
    "va_txt = [origin_txt[_idx] for _idx in va_idx]\n",
    "va_txt = [va_txt[j] for j in idx]\n",
    "va_txt = pd.DataFrame(va_txt, columns = ['txt'])\n",
    "y_pred_sa, A = sa(nd.array(va_x[idx], ctx = context))\n",
    "pred_sa = [nd.round(val).asnumpy()[0] for val in nd.softmax(y_pred_sa)[:, 1]] \n",
    "pred_sa_pd = pd.DataFrame(pred_sa, columns  = ['pred_sa'])\n",
    "label_pd = pd.DataFrame([va_y[j] for j in idx], columns = ['label'])\n",
    "result = pd.concat([va_txt, pred_sa_pd, label_pd], axis = 1)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "* Only 3 comments are mis-classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['pred_sa'] != result['label']].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(_att, _sentence, sentence_id):\n",
    "    x = _sentence[sentence_id]\n",
    "    _att = _att[sentence_id].asnumpy()\n",
    "    word = []\n",
    "    for token in x:\n",
    "        _word_idx = [i for i,j in enumerate(token) if j == 1]\n",
    "        _word = idx2word[_word_idx[0]]\n",
    "        word.append(_word)\n",
    "    print(type(_att))\n",
    "    att = pd.DataFrame(_att, index = word, columns = word)\n",
    "    return att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = get_attention(A, va_x[idx], sample_id)\n",
    "type(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "label: 1, predicted: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot using Seaborn\n",
    "sample_id = 2\n",
    "ax = sns.heatmap(get_attention(A, va_x[idx], sample_id))\n",
    "print('label: {}, predicted: {}'.format(result.label[sample_id], result.pred_sa[sample_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
