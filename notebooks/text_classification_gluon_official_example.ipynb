{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T04:50:44.844978Z",
     "start_time": "2018-07-09T04:50:43.819308Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T04:50:50.657503Z",
     "start_time": "2018-07-09T04:50:50.653000Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "num_gpus = 1\n",
    "learning_rate = .005 * num_gpus\n",
    "batch_size = 16 * num_gpus\n",
    "bucket_num = 10\n",
    "bucket_ratio = .2\n",
    "epochs = 1\n",
    "grad_clip = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T04:51:36.272448Z",
     "start_time": "2018-07-09T04:51:36.269059Z"
    }
   },
   "outputs": [],
   "source": [
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T04:52:49.602982Z",
     "start_time": "2018-07-09T04:52:49.470725Z"
    }
   },
   "outputs": [],
   "source": [
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T04:51:39.201427Z",
     "start_time": "2018-07-09T04:51:39.063105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200.0, TNC, num_layers=2)\n",
      "  (out_layer): HybridSequential(\n",
      "    (0): Dropout(p = 0, axes=())\n",
      "    (1): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SentimentNet(gluon.Block):\n",
    "    def __init__(self, embedding_block, encoder_block, dropout,\n",
    "                 prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = embedding_block\n",
    "            self.encoder = encoder_block\n",
    "            self.out_layer = gluon.nn.HybridSequential()\n",
    "            with self.out_layer.name_scope():\n",
    "                self.out_layer.add(gluon.nn.Dropout(dropout))\n",
    "                self.out_layer.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def forward(self, data, valid_length):\n",
    "        encoded = self.encoder(nd.Dropout(self.embedding(data),\n",
    "                                          0.2, axes=(0,)))  # Shape(T, N, C)\n",
    "        # Zero out the values with position exceeding the valid length.\n",
    "        masked_encoded = nd.SequenceMask(encoded,\n",
    "                                         sequence_length=valid_length,\n",
    "                                         use_sequence_length=True)\n",
    "        agg_state = nd.broadcast_div(nd.sum(masked_encoded, axis=0),\n",
    "                                     nd.expand_dims(valid_length, axis=1))\n",
    "        out = self.out_layer(agg_state)\n",
    "        return out\n",
    "\n",
    "\n",
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout)\n",
    "net = SentimentNet(embedding_block=lm_model.embedding,\n",
    "                   encoder_block=lm_model.encoder,\n",
    "                   dropout=dropout)\n",
    "net.out_layer.initialize(mx.init.Xavier(), ctx=context)\n",
    "net.hybridize()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spcy = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize using spaCy...\n",
      "Done! Tokenizing Time=1816.11s, #Sentences=25000\n",
      "Done! Tokenizing Time=1792.55s, #Sentences=25000\n"
     ]
    }
   ],
   "source": [
    "# train_dataset and test_dataset are both SimpleDataset objects,\n",
    "# which is a wrapper for lists and arrays.\n",
    "\n",
    "train_dataset, test_dataset = [nlp.data.IMDB(segment = segment) for segment in ('train', 'test')]\n",
    "print(\"Tokenize using spaCy...\")\n",
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "# length_clip takes as input a list and outputs a list with maximum length 500.\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "\n",
    "    data, label = x\n",
    "    # In the labeled train/test sets, a negative review has a score <= 4\n",
    "    # out of 10, and a positive review has a score >= 7 out of 10. Thus\n",
    "    # reviews with more neutral ratings are not included in the train/test\n",
    "    # sets. We labeled a negative review whose score <= 4 as 0, and a\n",
    "    # positive review whose score >= 7 as 1. As the neural ratings are not\n",
    "    # included in the datasets, we can simply use 5 as our threshold.\n",
    "    label = int(label > 5)\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    data = spcy(data)\n",
    "    data = length_clip([token.lemma_ for token in data if not token.is_stop])\n",
    "    return data, label, float(len(data))\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump([train_dataset_tkn, test_dataset_tkn], open('nouns.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    print('Begin Testing...')\n",
    "    for i, (data, label, valid_length) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net(data, valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader),\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(),\n",
    "                            'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack('float32'),\n",
    "                                          nlp.data.batchify.Stack('float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(train_data_lengths,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        num_buckets=bucket_num,\n",
    "                                                        ratio=bucket_ratio,\n",
    "                                                        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                             batch_sampler=batch_sampler,\n",
    "                                             batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            batchify_fn=batchify_fn)\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, (data, label, length) in enumerate(train_dataloader):\n",
    "            if data.shape[0] > len(context):\n",
    "                # Multi-gpu training.\n",
    "                data_list, label_list, length_list \\\n",
    "                = [gluon.utils.split_and_load(x,\n",
    "                                              context,\n",
    "                                              batch_axis=0,\n",
    "                                              even_split=False)\n",
    "                   for x in [data, label, length]]\n",
    "            else:\n",
    "                data_list = [data.as_in_context(context[0])]\n",
    "                label_list = [label.as_in_context(context[0])]\n",
    "                length_list = [length.as_in_context(context[0])]\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            for data, label, valid_length in zip(data_list, label_list, length_list):\n",
    "                valid_length = valid_length\n",
    "                with autograd.record():\n",
    "                    output = net(data.T, valid_length)\n",
    "                    L = L + loss(output, label).mean().as_in_context(context[0])\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm([p.grad(x.context)\n",
    "                                              for p in parameters for x in data_list],\n",
    "                                             grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print('[Epoch {} Batch {}/{}] elapsed {:.2f} s, \\\n",
    "                      avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, len(train_dataloader),\n",
    "                    time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context[0])\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, \\\n",
    "        test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "            epoch, epoch_L / epoch_sent_num,\n",
    "            test_acc, test_avg_L, epoch_wc / 1000 /\n",
    "            (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=25000, batch_num=1551\n",
      "  key=[59, 108, 157, 206, 255, 304, 353, 402, 451, 500]\n",
      "  cnt=[590, 1999, 5092, 5102, 3038, 2085, 1477, 1165, 870, 3582]\n",
      "  batch_size=[27, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
      "[Epoch 0 Batch 100/1551] elapsed 3.45 s,                       avg loss 0.002474, throughput 114.56K wps\n",
      "[Epoch 0 Batch 200/1551] elapsed 3.06 s,                       avg loss 0.002135, throughput 135.99K wps\n",
      "[Epoch 0 Batch 300/1551] elapsed 2.78 s,                       avg loss 0.002100, throughput 129.04K wps\n",
      "[Epoch 0 Batch 400/1551] elapsed 2.95 s,                       avg loss 0.001789, throughput 136.83K wps\n",
      "[Epoch 0 Batch 500/1551] elapsed 3.01 s,                       avg loss 0.001631, throughput 139.12K wps\n",
      "[Epoch 0 Batch 600/1551] elapsed 2.90 s,                       avg loss 0.001526, throughput 142.03K wps\n",
      "[Epoch 0 Batch 700/1551] elapsed 3.07 s,                       avg loss 0.001386, throughput 140.10K wps\n",
      "[Epoch 0 Batch 800/1551] elapsed 3.08 s,                       avg loss 0.001330, throughput 139.75K wps\n",
      "[Epoch 0 Batch 900/1551] elapsed 3.57 s,                       avg loss 0.001346, throughput 116.60K wps\n",
      "[Epoch 0 Batch 1000/1551] elapsed 2.83 s,                       avg loss 0.001522, throughput 126.01K wps\n",
      "[Epoch 0 Batch 1100/1551] elapsed 2.74 s,                       avg loss 0.001425, throughput 135.85K wps\n",
      "[Epoch 0 Batch 1200/1551] elapsed 2.77 s,                       avg loss 0.001350, throughput 141.44K wps\n",
      "[Epoch 0 Batch 1300/1551] elapsed 2.82 s,                       avg loss 0.001326, throughput 133.97K wps\n",
      "[Epoch 0 Batch 1400/1551] elapsed 2.78 s,                       avg loss 0.001355, throughput 137.65K wps\n",
      "[Epoch 0 Batch 1500/1551] elapsed 2.86 s,                       avg loss 0.001210, throughput 137.58K wps\n",
      "Begin Testing...\n",
      "[Batch 100/1563] elapsed 3.39 s\n",
      "[Batch 200/1563] elapsed 3.11 s\n",
      "[Batch 300/1563] elapsed 3.10 s\n",
      "[Batch 400/1563] elapsed 3.15 s\n",
      "[Batch 500/1563] elapsed 3.08 s\n",
      "[Batch 600/1563] elapsed 3.11 s\n",
      "[Batch 700/1563] elapsed 3.44 s\n",
      "[Batch 800/1563] elapsed 3.11 s\n",
      "[Batch 900/1563] elapsed 3.22 s\n",
      "[Batch 1000/1563] elapsed 3.16 s\n",
      "[Batch 1100/1563] elapsed 3.38 s\n",
      "[Batch 1200/1563] elapsed 3.20 s\n",
      "[Batch 1300/1563] elapsed 3.12 s\n",
      "[Batch 1400/1563] elapsed 3.17 s\n",
      "[Batch 1500/1563] elapsed 3.21 s\n",
      "[Epoch 0] train avg loss 0.001584, test acc 0.86,         test avg loss 0.321167, throughput 133.73K wps\n"
     ]
    }
   ],
   "source": [
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(mx.nd.reshape(mx.nd.array(vocab[['This', 'movie', 'is', 'amazing']],\n",
    "                              ctx=context[0]), shape=(-1, 1)),\n",
    "    mx.nd.array([4], ctx=context[0])).sigmoid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
