{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REF\n",
    "* https://github.com/dmlc/gluon-nlp/blob/master/docs/api/notes/data_api.rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-04T04:17:55.562224Z",
     "start_time": "2018-07-04T04:17:54.161752Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score, auc\n",
    "from mxnet import gluon\n",
    "\n",
    "\n",
    "import time, re\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import mxnet as mx\n",
    "import spacy\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20\n",
    "MAX_VOCAB = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T03:20:57.338763Z",
     "start_time": "2018-07-20T03:18:07.883505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count words and build vocab...\n",
      "Prepare data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yn</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yn  index\n",
       "0   0   3091\n",
       "1   1   3995"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "word_freq = collections.Counter()\n",
    "max_len = 0\n",
    "num_rec = 0\n",
    "print('Count words and build vocab...')\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _lab, _sen = line.decode('utf8').strip().split('\\t')\n",
    "        words = [token.lemma_ for token in nlp(_sen) if token.is_alpha] # Stop word제거 안한 상태 \n",
    "        # 제거를 위해 [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        if len(words) > max_len:\n",
    "            max_len = len(words)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        num_rec += 1\n",
    "\n",
    "# most_common output -> list\n",
    "word2idx = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(MAX_VOCAB - 2))}\n",
    "word2idx ['PAD'] = 0\n",
    "word2idx['UNK'] = 1\n",
    "\n",
    "idx2word= {i:v for v, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print('Prepare data...')\n",
    "y = []\n",
    "x = []\n",
    "origin_txt = []\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _label, _sen = line.decode('utf8').strip().split('\\t')\n",
    "        origin_txt.append(_sen)\n",
    "        y.append(int(_label))\n",
    "        words = [token.lemma_ for token in nlp(_sen) if token.is_alpha] # Stop word제거 안한 상태\n",
    "        words = [x for x in words if x != '-PRON-'] # '-PRON-' 제거\n",
    "        _seq = []\n",
    "        for word in words:\n",
    "            if word in word2idx.keys():\n",
    "                _seq.append(word2idx[word])\n",
    "            else:\n",
    "                _seq.append(word2idx['UNK'])\n",
    "        if len(_seq) < MAX_SENTENCE_LENGTH:\n",
    "            _seq.extend([0] * ((MAX_SENTENCE_LENGTH) - len(_seq)))\n",
    "        else:\n",
    "            _seq = _seq[:MAX_SENTENCE_LENGTH]\n",
    "        x.append(_seq)\n",
    "\n",
    "pd.DataFrame(y, columns = ['yn']).reset_index().groupby('yn').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T04:16:52.755123Z",
     "start_time": "2018-07-20T04:16:52.645286Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data process - tr/va split and define iterator\n",
    "\n",
    "tr_idx = np.random.choice(range(len(x)), int(len(x) * .8))\n",
    "va_idx = [x for x in range(len(x)) if x not in tr_idx]\n",
    "\n",
    "tr_x = [x[i] for i in tr_idx]\n",
    "tr_y = [y[i] for i in tr_idx]\n",
    "tr_origin = [origin_txt[i] for i in tr_idx]\n",
    "\n",
    "va_x = [x[i] for i in va_idx]\n",
    "va_y = [y[i] for i in va_idx]\n",
    "va_origin = [origin_txt[i] for i in va_idx]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "learning_rate = .0002\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "train_data = mx.io.NDArrayIter(data=[tr_x, tr_y], batch_size=batch_size, shuffle = False, last_batch_handle = 'discard')\n",
    "valid_data = mx.io.NDArrayIter(data=[va_x, va_y], batch_size=batch_size, shuffle = False, last_batch_handle = 'discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-20T04:16:57.940309Z",
     "start_time": "2018-07-20T04:16:57.935557Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.ndarray.linalg import gemm2\n",
    "import mxnet as mx\n",
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:32.970927Z",
     "start_time": "2018-07-21T14:17:32.955877Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sentence_Representation(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sentence_Representation, self).__init__()\n",
    "        for (k, v) in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "            self.g_fc1 = nn.Dense(self.hidden_dim,activation='relu')\n",
    "            self.g_fc2 = nn.Dense(self.hidden_dim,activation='relu')\n",
    "            \n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x) # batch * time step * embedding\n",
    "        x_i = embeds.expand_dims(1) # 64 * 1* 40 * 2000* : 0.02GB\n",
    "        x_i = nd.repeat(x_i,repeats= self.sentence_length, axis=1) # 64 * 40 * 40 * 2000: 1.52GB\n",
    "        x_j = embeds.expand_dims(2) # 64 * 40 * 1 * 2000\n",
    "        x_j = nd.repeat(x_j,repeats= self.sentence_length, axis=2) # 64 * 40 * 40 * 2000: 1.52GB\n",
    "        x_full = nd.concat(x_i,x_j,dim=3) # 64 * 40 * 40 * 4000: 3.04GB\n",
    "        # batch*sentence_length*sentence_length개의 batch를 가진 2*VOCABULARY input을 network에 feed\n",
    "        _x = x_full.reshape((-1, 2 * self.emb_dim))\n",
    "        \n",
    "        _x = self.g_fc1(_x) # (64 * 40 * 40) * 256: .1GB 추가메모리는 안먹나?\n",
    "        _x = self.g_fc2(_x) # (64 * 40 * 40) * 256: .1GB (reuse)\n",
    "        # sentence_length*sentence_length개의 결과값을 모두 합해서 sentence representation으로 나타냄\n",
    "        x_g = _x.reshape((-1, self.sentence_length * self.sentence_length, self.hidden_dim)) # (64, 40*40, 256) : .1GB\n",
    "        sentence_rep = x_g.sum(1) # (64, 256): ignorable\n",
    "        return sentence_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential()\n",
    "classifier.add(nn.Dense(16, activation = 'relu'))\n",
    "classifier.add(nn.Dense(8, activation = 'relu'))\n",
    "classifier.add(nn.Dense(1))\n",
    "classifier.collect_params().initialize(mx.init.Xavier(), ctx = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:35.867152Z",
     "start_time": "2018-07-21T14:17:35.858191Z"
    }
   },
   "outputs": [],
   "source": [
    "class SA_Classifier(nn.Block):\n",
    "    def __init__(self, sen_rep, classifier, context, **kwargs):\n",
    "        super(SA_Classifier, self).__init__(**kwargs)\n",
    "        self.context = context\n",
    "        with self.name_scope():\n",
    "            self.sen_rep = sen_rep\n",
    "            self.classifier = classifier\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Initial hidden state\n",
    "        # sentence representation할 때 hidden의 context가 cpu여서 오류 발생. context를 gpu로 전환\n",
    "        x = self.sen_rep(x)\n",
    "        x = nd.flatten(x)\n",
    "        res = self.classifier(x)\n",
    "        return res       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiate sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:38.550227Z",
     "start_time": "2018-07-21T14:17:38.536795Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 10 # Emb dim\n",
    "hidden_dim = 30 # Hidden dim for LSTM\n",
    "\n",
    "param = {'emb_dim': emb_dim, 'hidden_dim': hidden_dim, 'vocab_size': vocab_size, 'sentence_length': MAX_SENTENCE_LENGTH, 'dropout': .2}\n",
    "sen_rep = Sentence_Representation(**param)\n",
    "sen_rep.collect_params().initialize(mx.init.Xavier(), ctx = context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:39.912316Z",
     "start_time": "2018-07-21T14:17:39.906486Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/mxnet/gluon/nn/basic_layers.py:83: UserWarning: All children of this Sequential layer are HybridBlocks. Consider using HybridSequential for the best performance.\n",
      "  warnings.warn('All children of this Sequential layer are HybridBlocks. Consider ' \\\n"
     ]
    }
   ],
   "source": [
    "sa = SA_Classifier(sen_rep, classifier, context)\n",
    "loss = gluon.loss.SigmoidBCELoss()\n",
    "trainer = gluon.Trainer(sa.collect_params(), 'adam', {'learning_rate': 1e-3})\n",
    "sa.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:17:41.166470Z",
     "start_time": "2018-07-21T14:17:41.155606Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, dataIterator, context):\n",
    "    dataIterator.reset()\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    for i, batch in enumerate(dataIterator):\n",
    "        data =  batch.data[0].as_in_context(context)\n",
    "        label = batch.data[1].as_in_context(context)\n",
    "        output = net(data)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape((-1,))\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += len(label)\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, dataIterator.num_data//dataIterator.batch_size,\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:36:02.670468Z",
     "start_time": "2018-07-21T14:35:18.885713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7978f97ad64271b44e720995c74ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/354] elapsed 0.66 s,                     avg loss 0.000885, throughput 2.41K wps\n",
      "[Epoch 0 Batch 200/354] elapsed 0.59 s,                     avg loss 0.000228, throughput 2.69K wps\n",
      "[Epoch 0 Batch 300/354] elapsed 0.59 s,                     avg loss 0.000617, throughput 2.70K wps\n",
      "[Batch 100/197] elapsed 0.26 s\n",
      "[Epoch 0] train avg loss 0.000512, valid acc 0.99,         valid avg loss 0.040986, throughput 2.61K wps\n",
      "[Epoch 1 Batch 100/354] elapsed 0.59 s,                     avg loss 0.000065, throughput 2.69K wps\n",
      "[Epoch 1 Batch 200/354] elapsed 0.59 s,                     avg loss 0.000022, throughput 2.69K wps\n",
      "[Epoch 1 Batch 300/354] elapsed 0.59 s,                     avg loss 0.000417, throughput 2.69K wps\n",
      "[Batch 100/197] elapsed 0.25 s\n",
      "[Epoch 1] train avg loss 0.000153, valid acc 0.99,         valid avg loss 0.057481, throughput 2.69K wps\n",
      "[Epoch 2 Batch 100/354] elapsed 0.59 s,                     avg loss 0.000429, throughput 2.69K wps\n",
      "[Epoch 2 Batch 200/354] elapsed 0.59 s,                     avg loss 0.000285, throughput 2.70K wps\n",
      "[Epoch 2 Batch 300/354] elapsed 0.60 s,                     avg loss 0.000116, throughput 2.69K wps\n",
      "[Batch 100/197] elapsed 0.25 s\n",
      "[Epoch 2] train avg loss 0.000237, valid acc 0.99,         valid avg loss 0.050732, throughput 2.69K wps\n",
      "[Epoch 3 Batch 100/354] elapsed 0.59 s,                     avg loss 0.000226, throughput 2.69K wps\n",
      "[Epoch 3 Batch 200/354] elapsed 0.59 s,                     avg loss 0.000039, throughput 2.70K wps\n",
      "[Epoch 3 Batch 300/354] elapsed 0.59 s,                     avg loss 0.000002, throughput 2.69K wps\n",
      "[Batch 100/197] elapsed 0.25 s\n",
      "[Epoch 3] train avg loss 0.000076, valid acc 0.99,         valid avg loss 0.068538, throughput 2.69K wps\n",
      "[Epoch 4 Batch 100/354] elapsed 0.59 s,                     avg loss 0.000011, throughput 2.70K wps\n",
      "[Epoch 4 Batch 200/354] elapsed 0.60 s,                     avg loss 0.000001, throughput 2.69K wps\n",
      "[Epoch 4 Batch 300/354] elapsed 0.59 s,                     avg loss 0.000001, throughput 2.69K wps\n",
      "[Batch 100/197] elapsed 0.25 s\n",
      "[Epoch 4] train avg loss 0.000004, valid acc 0.99,         valid avg loss 0.074657, throughput 2.69K wps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 5\n",
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    # Epoch training stats\n",
    "    start_epoch_time = time.time()\n",
    "    epoch_L = 0.0\n",
    "    epoch_sent_num = 0\n",
    "    epoch_wc = 0\n",
    "    # Log interval training stats\n",
    "    start_log_interval_time = time.time()\n",
    "    log_interval_wc = 0\n",
    "    log_interval_sent_num = 0\n",
    "    log_interval_L = 0.0\n",
    "    \n",
    "    for i, batch in enumerate(train_data):\n",
    "        _data = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        L = 0\n",
    "        wc = len(_data)\n",
    "        log_interval_wc += wc\n",
    "        epoch_wc += wc\n",
    "        log_interval_sent_num += _data.shape[1]\n",
    "        epoch_sent_num += _data.shape[1]\n",
    "        with autograd.record():\n",
    "            _out = sa(_data)\n",
    "            L = L + loss(_out, _label).mean().as_in_context(context)\n",
    "        L.backward()\n",
    "        trainer.step(_data.shape[0])\n",
    "        log_interval_L += L.asscalar()\n",
    "        epoch_L += L.asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            tqdm.write('[Epoch {} Batch {}/{}] elapsed {:.2f} s, \\\n",
    "                    avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, train_data.num_data//train_data.batch_size,\n",
    "                    time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num,\n",
    "                    log_interval_wc / 1000 / (time.time() - start_log_interval_time)))\n",
    "            # Clear log interval training stats\n",
    "            start_log_interval_time = time.time()\n",
    "            log_interval_wc = 0\n",
    "            log_interval_sent_num = 0\n",
    "            log_interval_L = 0\n",
    "    end_epoch_time = time.time()\n",
    "    test_avg_L, test_acc = evaluate(sa, valid_data, context)\n",
    "    tqdm.write('[Epoch {}] train avg loss {:.6f}, valid acc {:.2f}, \\\n",
    "        valid avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "        epoch, epoch_L / epoch_sent_num,\n",
    "        test_acc, test_avg_L, epoch_wc / 1000 /\n",
    "        (end_epoch_time - start_epoch_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:03.714909Z",
     "start_time": "2018-07-21T14:30:03.704239Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pred(net, iterator):\n",
    "    pred_sa = []\n",
    "    label_sa = []\n",
    "    va_text = []\n",
    "    iterator.reset()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        if i % 100 == 0:\n",
    "            print('i = {}'.format(i))\n",
    "        data =  batch.data[0].as_in_context(context)\n",
    "        label = batch.data[1].as_in_context(context)\n",
    "        output = net(data)\n",
    "        L = loss(output, label)\n",
    "        pred = (nd.sigmoid(output) > 0.5).reshape((-1,))\n",
    "        pred_sa.extend(pred.asnumpy())\n",
    "        label_sa.extend(label.asnumpy())\n",
    "        va_text.extend([' '.join([idx2word[np.int(x)] for x in y.asnumpy() if idx2word[np.int(x)] is not 'PAD']) for y in data])\n",
    "    pred_sa_pd = pd.DataFrame(pred_sa, columns  = ['pred_sa'])\n",
    "    label_pd = pd.DataFrame(label_sa, columns = ['label'])\n",
    "    text_pd = pd.DataFrame(va_text, columns = ['text'])\n",
    "    res = pd.concat([text_pd, pred_sa_pd, label_pd], axis = 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:17.312238Z",
     "start_time": "2018-07-21T14:30:06.907985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 100\n"
     ]
    }
   ],
   "source": [
    "result = get_pred(sa, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:27.564873Z",
     "start_time": "2018-07-21T14:30:27.555956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-21T14:30:43.777958Z",
     "start_time": "2018-07-21T14:30:43.764692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred_sa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the da vinci code be a really good movie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>personally neither hate nor love the da vinci ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>see both da vinci code and x men the last stan...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>then go to the movie and see the da vinci code...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>ok time to update wow have update for a long t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>mission impossible do kick ass and yes jessica...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>like mission impossible but hate tom cruise ge...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>child text fantasy perhaps most obviously be o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>love harry potter book not the bootleg movie w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>write a harry potter poem for a chance to win ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  pred_sa  label\n",
       "9             the da vinci code be a really good movie      0.0    1.0\n",
       "22   personally neither hate nor love the da vinci ...      0.0    1.0\n",
       "73   see both da vinci code and x men the last stan...      0.0    1.0\n",
       "82   then go to the movie and see the da vinci code...      0.0    1.0\n",
       "438  ok time to update wow have update for a long t...      0.0    1.0\n",
       "520  mission impossible do kick ass and yes jessica...      0.0    1.0\n",
       "524  like mission impossible but hate tom cruise ge...      0.0    1.0\n",
       "903  child text fantasy perhaps most obviously be o...      0.0    1.0\n",
       "910  love harry potter book not the bootleg movie w...      0.0    1.0\n",
       "914  write a harry potter poem for a chance to win ...      0.0    1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'personally neither hate nor love the da vinci code'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result.pred_sa != result.label]['text'].iloc[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
