{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/kookmin/py_libs/lib/python3.6/site-packages (0.72)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.6/site-packages (from xgboost) (1.14.5)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.6/site-packages (from xgboost) (1.1.0)\n",
      "\u001b[31mmxnet-cu80 1.1.0 has requirement numpy<=1.13.3, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
      "Requirement already satisfied: nltk in /home/kookmin/py_libs/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[31mmxnet-cu80 1.1.0 has requirement numpy<=1.13.3, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
      "Collecting tqdm\n",
      "  Using cached https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl\n",
      "\u001b[31mmxnet-cu80 1.1.0 has requirement numpy<=1.13.3, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.23.4\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost --user\n",
    "! pip install nltk --user\n",
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:37:44.354755Z",
     "start_time": "2018-06-16T16:37:43.892861Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:37:46.274369Z",
     "start_time": "2018-06-16T16:37:44.960453Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freq = collections.Counter()\n",
    "max_len = 0\n",
    "num_rec = 0\n",
    "\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        label, sentence = line.decode('utf8').strip().split('\\t')\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        if len(words) > max_len:\n",
    "            max_len = len(words)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        num_rec += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:37:46.648243Z",
     "start_time": "2018-06-16T16:37:46.637958Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "# most_common output -> list\n",
    "word2idx = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(MAX_FEATURES - 2))}\n",
    "word2idx ['PAD'] = 0\n",
    "word2idx['UNK'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:37:47.495361Z",
     "start_time": "2018-06-16T16:37:47.490976Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2word= {i:v for v, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:37:49.787677Z",
     "start_time": "2018-06-16T16:37:48.425500Z"
    }
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "origin_txt = []\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _label, _sentence = line.decode('utf8').strip().split('\\t')\n",
    "        origin_txt.append(_sentence)\n",
    "        y.append(int(_label))\n",
    "        words = nltk.word_tokenize(_sentence.lower())\n",
    "        _seq = []\n",
    "        for word in words:\n",
    "            if word in word2idx.keys():\n",
    "                _seq.append(word2idx[word])\n",
    "            else:\n",
    "                _seq.append(word2idx['UNK'])\n",
    "        if len(_seq) < MAX_SENTENCE_LENGTH:\n",
    "            _seq.extend([0] * ((MAX_SENTENCE_LENGTH) - len(_seq)))\n",
    "        else:\n",
    "            _seq = _seq[:MAX_SENTENCE_LENGTH]\n",
    "        x.append(_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:37:53.299224Z",
     "start_time": "2018-06-16T16:37:53.252312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yn</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yn  index\n",
       "0   0   3091\n",
       "1   1   3995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y, columns = ['yn']).reset_index().groupby('yn').count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence representation: Average of BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:37:57.914770Z",
     "start_time": "2018-06-16T16:37:57.910762Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot(x, vocab_size):\n",
    "    res = np.zeros(shape = (vocab_size))\n",
    "    res[x] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:00.839992Z",
     "start_time": "2018-06-16T16:37:58.944700Z"
    }
   },
   "outputs": [],
   "source": [
    "x_1 = np.array([np.sum(np.array([one_hot(word, MAX_FEATURES) for word in example]), axis = 0) for example in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process - tr/va split and define iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:02.138524Z",
     "start_time": "2018-06-16T16:38:02.052616Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_idx = np.random.choice(range(x_1.shape[0]), int(x_1.shape[0] * .8))\n",
    "va_idx = [x for x in range(x_1.shape[0]) if x not in tr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:03.827339Z",
     "start_time": "2018-06-16T16:38:03.637716Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_x = x_1[tr_idx, :]\n",
    "tr_y = [y[i] for i in tr_idx]\n",
    "va_x = x_1[va_idx, :]\n",
    "va_y = [y[i] for i in va_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "* If we transform sentence into machine-understandable form via average of BOW, we can separate representation and classification\n",
    "* Here, we will apply various classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:20:27.240271Z",
     "start_time": "2018-06-16T16:20:27.222750Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:21:45.266083Z",
     "start_time": "2018-06-16T16:21:24.711438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(tr_x, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:22:11.861052Z",
     "start_time": "2018-06-16T16:22:11.679670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred_xgb = xgb.predict(va_x)\n",
    "pred_xgb = [round(val) for val in y_pred_xgb]\n",
    "\n",
    "# Check predictions\n",
    "#pred_pd= pd.DataFrame(pred_xgb, columns = ['pred']).reset_index()\n",
    "#pred_pd.groupby(['pred']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:22:54.227408Z",
     "start_time": "2018-06-16T16:22:54.219056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "accuracy_xgb = accuracy_score(va_y, pred_xgb)\n",
    "print('Accuracy: %.2f%%'%(accuracy_xgb * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:22:59.099704Z",
     "start_time": "2018-06-16T16:22:59.064160Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:23:02.455789Z",
     "start_time": "2018-06-16T16:23:02.046982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(tr_x, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:23:04.872970Z",
     "start_time": "2018-06-16T16:23:04.813480Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_rf = rf.predict(va_x)\n",
    "pred_rf = [round(val) for val in y_pred_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:23:06.935466Z",
     "start_time": "2018-06-16T16:23:06.927963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.10%\n"
     ]
    }
   ],
   "source": [
    "accuracy_rf = accuracy_score(va_y, pred_rf)\n",
    "print('Accuracy: %.2f%%'%(accuracy_rf * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:23:08.901361Z",
     "start_time": "2018-06-16T16:23:08.897570Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:27:36.872185Z",
     "start_time": "2018-06-16T16:27:36.865713Z"
    }
   },
   "outputs": [],
   "source": [
    "models = (svm.SVC(kernel = 'linear', C = 1.0), # C: SVM Regularization parameter\n",
    "          svm.LinearSVC(C = 1.0),\n",
    "          svm.SVC(kernel = 'rbf', gamma = .7, C = 1.0),\n",
    "          svm.SVC(kernel = 'poly', degree = 3, C = 1.0)\n",
    ")\n",
    "\n",
    "models = (mdl.fit(tr_x, tr_y) for mdl in models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:30:16.093817Z",
     "start_time": "2018-06-16T16:27:40.386965Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_svm = (mdl.predict(va_x) for mdl in models)\n",
    "pred_svm = [[round(val) for val in _pred] for _pred in y_pred_svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:31:25.546462Z",
     "start_time": "2018-06-16T16:31:25.528584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [99.37 99.37 93.86 58.1 ]\n"
     ]
    }
   ],
   "source": [
    "accuracy_svm = [accuracy_score(va_y, pred) for pred in pred_svm]\n",
    "print('Accuracy: {}'.format(np.round(accuracy_svm, 4)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:35:16.199066Z",
     "start_time": "2018-06-16T16:35:16.144108Z"
    }
   },
   "outputs": [],
   "source": [
    "va_txt = pd.DataFrame(np.array([origin_txt[idx] for idx in va_idx]), columns = ['txt'])\n",
    "pred_rf_pd = pd.DataFrame(pred_rf, columns  = ['pred_rf'])\n",
    "pred_xgb_pd = pd.DataFrame(pred_xgb, columns  = ['pred_xgb'])\n",
    "pred_svm_svc_pd = pd.DataFrame(pred_svm[2], columns  = ['pred_svm'])\n",
    "label_pd = pd.DataFrame(va_y, columns = ['label'])\n",
    "result = pd.concat([va_txt, pred_rf_pd, pred_xgb_pd, pred_svm_svc_pd, label_pd], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:35:17.654932Z",
     "start_time": "2018-06-16T16:35:17.639960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>pred_rf</th>\n",
       "      <th>pred_xgb</th>\n",
       "      <th>pred_svm</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I loved the Da Vinci Code, but now I want some...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 txt  pred_rf  pred_xgb  \\\n",
       "0  this was the first clive cussler i've ever rea...        1         1   \n",
       "1                   i liked the Da Vinci Code a lot.        1         1   \n",
       "2  I liked the Da Vinci Code but it ultimatly did...        0         1   \n",
       "3  that's not even an exaggeration ) and at midni...        1         0   \n",
       "4  I loved the Da Vinci Code, but now I want some...        0         1   \n",
       "\n",
       "   pred_svm  label  \n",
       "0         1      1  \n",
       "1         1      1  \n",
       "2         1      1  \n",
       "3         1      1  \n",
       "4         1      1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:35:20.710584Z",
     "start_time": "2018-06-16T16:35:20.698418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of error case 60\n",
      "# of error case 59\n",
      "# of error case 194\n"
     ]
    }
   ],
   "source": [
    "print('# of error case {}'.format(result[result['pred_rf'] != result['label']].shape[0]))\n",
    "print('# of error case {}'.format(result[result['pred_xgb'] != result['label']].shape[0]))\n",
    "print('# of error case {}'.format(result[result['pred_svm'] != result['label']].shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:11.580851Z",
     "start_time": "2018-06-16T16:38:11.082714Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:12.303080Z",
     "start_time": "2018-06-16T16:38:12.297603Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, input_dim, emb_dim, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(input_dim = input_dim, output_dim = emb_dim)\n",
    "            self.dense1 = nn.Dense(64)\n",
    "            #self.dense2 = nn.Dense(32, activation = 'relu')\n",
    "            self.bn = nn.BatchNorm()\n",
    "            self.dense2 = nn.Dense(2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn(x)\n",
    "        x = nd.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:13.587224Z",
     "start_time": "2018-06-16T16:38:13.582092Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_f(label, pred):\n",
    "    pred = pred.ravel()\n",
    "    label = label.ravel()\n",
    "    #print('pred = {}'.format(pred))\n",
    "    #print('label = {}'.format(label))\n",
    "    corr = ((pred > 0.5) == label)*1.\n",
    "    return (((pred > 0.5) == label)*1.).mean()\n",
    "tr_metric = mx.metric.CustomMetric(acc_f)\n",
    "va_metric = mx.metric.CustomMetric(acc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:20.656050Z",
     "start_time": "2018-06-16T16:38:20.647311Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "batch_size = 64\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T16:38:31.105063Z",
     "start_time": "2018-06-16T16:38:30.894757Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = mx.io.NDArrayIter(data=[tr_x, tr_y], batch_size=batch_size, shuffle = False)\n",
    "valid_data = mx.io.NDArrayIter(data=[va_x, va_y], batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-16T16:39:22.206Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLP(input_dim = MAX_FEATURES, emb_dim = 50)\n",
    "mlp.collect_params().initialize(mx.init.Xavier(), ctx = context)\n",
    "loss = gluon.loss.SoftmaxCELoss()\n",
    "trainer = gluon.Trainer(mlp.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a8e67a2d7942bd94d55d8f9beb7445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: tr_loss = [0.08059314], tr_acc= 0.9685744382022472, va_loss = \n",
      "[0.21987945]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.901875\n",
      "Epoch 1: tr_loss = [0.00654242], tr_acc= 0.999122191011236, va_loss = \n",
      "[0.36589307]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.8521875\n",
      "Epoch 2: tr_loss = [0.00178501], tr_acc= 0.9996488764044944, va_loss = \n",
      "[0.04059093]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.98875\n",
      "Epoch 3: tr_loss = [0.00112575], tr_acc= 0.9996488764044944, va_loss = \n",
      "[0.05118411]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.9878125\n",
      "Epoch 4: tr_loss = [0.00083126], tr_acc= 0.9996488764044944, va_loss = \n",
      "[0.0437212]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.98875\n",
      "Epoch 5: tr_loss = [0.00065467], tr_acc= 0.9996488764044944, va_loss = \n",
      "[0.04577913]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.98875\n",
      "Epoch 6: tr_loss = [0.00053728], tr_acc= 0.9998244382022472, va_loss = \n",
      "[0.04147633]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.98875\n",
      "Epoch 7: tr_loss = [0.00044258], tr_acc= 0.9998244382022472, va_loss = \n",
      "[0.04616626]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.9884375\n",
      "Epoch 8: tr_loss = [0.0003554], tr_acc= 1.0, va_loss = \n",
      "[0.04012142]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.99\n",
      "Epoch 9: tr_loss = [0.00030272], tr_acc= 1.0, va_loss = \n",
      "[0.0388553]\n",
      "<NDArray 1 @gpu(0)>, va_acc= 0.9896875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(train_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        with autograd.record():\n",
    "            _out = mlp(_dat)\n",
    "            _los = nd.sum(loss(_out, _label)) # 배치의 크기만큼의 loss가 나옴\n",
    "            _los.backward()\n",
    "        trainer.step(_dat.shape[0])\n",
    "        n_obs += _dat.shape[0]\n",
    "        #print(n_obs)\n",
    "        _total_los += nd.sum(_los).asnumpy()\n",
    "        # Epoch loss를 구하기 위해서 결과물을 계속 쌓음\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy()) # 두번째 컬럼의 확률이 예측 확률\n",
    "        label.extend(_label.asnumpy())\n",
    "    #print(pred)\n",
    "    #print([round(p) for p in pred]) # 기본이 float임\n",
    "    #print(label)\n",
    "    #print('**** ' + str(n_obs))\n",
    "    #print(label[:10])\n",
    "    #print(pred[:10])\n",
    "    #print([round(p) for p in pred][:10])\n",
    "    tr_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    tr_loss = _total_los/n_obs\n",
    "    \n",
    "    ### Evaluate training\n",
    "    valid_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(valid_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        _out = mlp(_dat)\n",
    "        _pred_score = nd.softmax(_out)\n",
    "        n_obs += _dat.shape[0]\n",
    "        _total_los += nd.sum(loss(_out, _label)).asnumpy()\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy())\n",
    "        label.extend(_label.asnumpy())\n",
    "    va_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    va_loss = _total_los/n_obs\n",
    "    tqdm.write('Epoch {}: tr_loss = {}, tr_acc= {}, va_loss = {}, va_acc= {}'.format(epoch, tr_loss, tr_acc, va_loss, va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp(nd.array(va_x, ctx = context))\n",
    "# softmax를 적용하고\n",
    "# 두번째 열을 뽑아와서\n",
    "# nd.round 함수를 적용해서 0/1 예측값을 얻고\n",
    "# numpy array로 바꾸고\n",
    "# 첫번째 원소를 뽑아서 예측 label로 사용\n",
    "pred_mlp = [nd.round(val).asnumpy()[0] for val in nd.softmax(y_pred_mlp)[:, 1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.77%\n"
     ]
    }
   ],
   "source": [
    "accuracy_mlp = accuracy_score(va_y, pred_mlp)\n",
    "print('Accuracy: %.2f%%'%(accuracy_rf * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN without embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = nn.Dense(64)\n",
    "            #self.dense2 = nn.Dense(32, activation = 'relu')\n",
    "            self.bn = nn.BatchNorm()\n",
    "            self.dense2 = nn.Dense(2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn(x)\n",
    "        x = nd.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "batch_size = 64\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_no_embedding = MLP()\n",
    "mlp_no_embedding.collect_params().initialize(mx.init.Xavier(), ctx = context)\n",
    "loss = gluon.loss.SoftmaxCELoss()\n",
    "trainer = gluon.Trainer(mlp_no_embedding.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8d873599e8454fa2cf4f3b09636c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: tr_loss = [0.1989796], tr_acc= 0.9373244382022472, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 1: tr_loss = [0.02037179], tr_acc= 0.9964887640449438, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 2: tr_loss = [0.00560753], tr_acc= 0.9996488764044944, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 3: tr_loss = [0.00271664], tr_acc= 1.0, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 4: tr_loss = [0.00167191], tr_acc= 1.0, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 5: tr_loss = [0.00115814], tr_acc= 1.0, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 6: tr_loss = [0.00085909], tr_acc= 1.0, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 7: tr_loss = [0.00066683], tr_acc= 1.0, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 8: tr_loss = [0.00053454], tr_acc= 1.0, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "Epoch 9: tr_loss = [0.00043856], tr_acc= 1.0, va_loss = [0.01757846], va_acc= 0.9953125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(train_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        with autograd.record():\n",
    "            _out = mlp_no_embedding(_dat)\n",
    "            _los = nd.sum(loss(_out, _label)) # 배치의 크기만큼의 loss가 나옴\n",
    "            _los.backward()\n",
    "        trainer.step(_dat.shape[0])\n",
    "        n_obs += _dat.shape[0]\n",
    "        #print(n_obs)\n",
    "        _total_los += nd.sum(_los).asnumpy()\n",
    "        # Epoch loss를 구하기 위해서 결과물을 계속 쌓음\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy()) # 두번째 컬럼의 확률이 예측 확률\n",
    "        label.extend(_label.asnumpy())\n",
    "    #print(pred)\n",
    "    #print([round(p) for p in pred]) # 기본이 float임\n",
    "    #print(label)\n",
    "    #print('**** ' + str(n_obs))\n",
    "    #print(label[:10])\n",
    "    #print(pred[:10])\n",
    "    #print([round(p) for p in pred][:10])\n",
    "    tr_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    tr_loss = _total_los/n_obs\n",
    "    \n",
    "    ### Evaluate training\n",
    "    valid_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(valid_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        _out = mlp(_dat)\n",
    "        _pred_score = nd.softmax(_out)\n",
    "        n_obs += _dat.shape[0]\n",
    "        _total_los += nd.sum(loss(_out, _label)).asnumpy()\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy())\n",
    "        label.extend(_label.asnumpy())\n",
    "    va_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    va_loss = _total_los/n_obs\n",
    "    tqdm.write('Epoch {}: tr_loss = {}, tr_acc= {}, va_loss = {}, va_acc= {}'.format(epoch, tr_loss, tr_acc, va_loss, va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp_no_embedding = mlp_no_embedding(nd.array(va_x, ctx = context))\n",
    "# softmax를 적용하고\n",
    "# 두번째 열을 뽑아와서\n",
    "# nd.round 함수를 적용해서 0/1 예측값을 얻고\n",
    "# numpy array로 바꾸고\n",
    "# 첫번째 원소를 뽑아서 예측 label로 사용\n",
    "pred_mlp_no_embedding = [nd.round(val).asnumpy()[0] for val in nd.softmax(y_pred_mlp)[:, 1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.77%\n"
     ]
    }
   ],
   "source": [
    "accuracy_mlp_no_embedding = accuracy_score(va_y, pred_mlp_no_embedding)\n",
    "print('Accuracy: %.2f%%'%(accuracy_rf * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_txt = pd.DataFrame(np.array([origin_txt[idx] for idx in va_idx]), columns = ['txt'])\n",
    "pred_mlp_no_embedding_pd = pd.DataFrame(pred_mlp_no_embedding, columns  = ['pred_mlp_no_embedding'])\n",
    "label_pd = pd.DataFrame(va_y, columns = ['label'])\n",
    "result = pd.concat([va_txt, pred_mlp_no_embedding_pd, label_pd], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['pred_mlp_no_embedding'] != result['label']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[9.9998224e-01 3.8912403e-05 1.5123112e-02 1.5123112e-02 3.8741960e-03\n",
       " 5.3047344e-05 4.4773667e-04 2.1941181e-02 1.3443723e-05 9.7836293e-02\n",
       " 5.4693965e-06 3.0347016e-03 4.5603651e-04 2.1782002e-04 9.1873727e-07\n",
       " 5.9484207e-04 1.2958754e-05 2.3370181e-05 4.5389035e-05 2.3826879e-06\n",
       " 2.0143841e-03 1.9676549e-05 3.8608381e-05 1.9259760e-05 2.3370181e-05\n",
       " 5.3492450e-04 1.4647118e-05 4.2287334e-06 5.1647483e-04 4.2883765e-02\n",
       " 2.4010626e-08 9.0014125e-04 1.9676549e-05 6.7549786e-07 6.2480030e-05\n",
       " 2.3370181e-05 8.4131352e-05 2.1141273e-05 9.9764225e-07 1.2948031e-05\n",
       " 2.2955202e-04 1.4018232e-03 3.2890926e-05 1.1850135e-01 5.3150420e-06\n",
       " 4.8941998e-05 2.3758446e-05 1.6405782e-05 3.2890926e-05 2.2077575e-04\n",
       " 4.2800168e-03 5.4771994e-05 3.9444578e-05 1.3237479e-04 9.3760107e-05\n",
       " 1.6405782e-05 9.8145443e-05 6.3716164e-03 3.9815055e-05 3.4375687e-04\n",
       " 3.2613567e-05 1.5314947e-05 2.9872683e-05 2.5956588e-05]\n",
       "<NDArray 64 @gpu(0)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pred_score[:, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
