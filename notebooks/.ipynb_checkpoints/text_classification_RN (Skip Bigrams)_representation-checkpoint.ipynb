{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = collections.Counter()\n",
    "max_len = 0\n",
    "num_rec = 0\n",
    "\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        label, sentence = line.decode('utf8').strip().split('\\t')\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        if len(words) > max_len:\n",
    "            max_len = len(words)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        num_rec += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, and Brokeback Mountain was a terrible movie.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "# most_common output -> list\n",
    "word2idx = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(MAX_FEATURES - 2))}\n",
    "word2idx ['PAD'] = 0\n",
    "word2idx['UNK'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word= {i:v for v, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []\n",
    "origin_txt = []\n",
    "with open('../data/umich-sentiment-train.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        _label, _sentence = line.decode('utf8').strip().split('\\t')\n",
    "        origin_txt.append(_sentence)\n",
    "        y.append(int(_label))\n",
    "        words = nltk.word_tokenize(_sentence.lower())\n",
    "        _seq = []\n",
    "        for word in words:\n",
    "            if word in word2idx.keys():\n",
    "                _seq.append(word2idx[word])\n",
    "            else:\n",
    "                _seq.append(word2idx['UNK'])\n",
    "        if len(_seq) < MAX_SENTENCE_LENGTH:\n",
    "            _seq.extend([0] * ((MAX_SENTENCE_LENGTH) - len(_seq)))\n",
    "        else:\n",
    "            _seq = _seq[:MAX_SENTENCE_LENGTH]\n",
    "        x.append(_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, vocab_size):\n",
    "    res = np.zeros(shape = (vocab_size))\n",
    "    res[x] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Iterator 만들기\n",
    "  * x :$ (batch \\times vocab \\times word)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.array([np.array([one_hot(word, MAX_FEATURES) for word in example]) for example in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx = np.random.choice(range(x_1.shape[0]), int(x_1.shape[0] * .8))\n",
    "va_idx = [x for x in range(x_1.shape[0]) if x not in tr_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x = x_1[tr_idx, :]\n",
    "tr_y = [y[i] for i in tr_idx]\n",
    "va_x = x_1[va_idx, :]\n",
    "va_y = [y[i] for i in va_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "batch_size = 16\n",
    "train_data = mx.io.NDArrayIter(data=[tr_x, tr_y], batch_size=batch_size, shuffle = False)\n",
    "valid_data = mx.io.NDArrayIter(data=[va_x, va_y], batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### float, int\n",
    "\n",
    "* int, float, (unicode) -> 4 Byte\n",
    "* list overhead -> 64 Byte\n",
    "* int, float overhead -> 24 Byte\n",
    "* 모든 primative도 객체로 인식해서 모두 20 Byte의 overhead가 붙음\n",
    "  - np.float16: 26 Byte (24 + 2 Byte)\n",
    "  - np.float32: 28 Byte (24 + 4 Byte)\n",
    "  - np.float64: 32 Byte (24 + 8 Byte)\n",
    "  \n",
    " * np.float32([5]).nbytes => 4 Byte\n",
    " * np.float32([5, 5]).nbytes => 8 Byte\n",
    "\n",
    " #### Byte\n",
    " \n",
    " * sys.getsizeof(b'') => 33 Byte\n",
    " * sys.getsizeof(b'5') => 34 Byte (원소가 1개 추가될 때마다 1 Byte씩 증가)\n",
    " \n",
    " #### Unicode\n",
    " \n",
    " * sys.getsizeof(u'') => 49 Byte\n",
    " * sys.getsizeof(u'5') => 58 Byte (원소가 1개 추가될 때 9 Byte 증가)\n",
    " * sys.getsizeof(u'56') => 51 Byte (원소가 1개 추가될 때마다 1 Byte씩 증가)\n",
    " * sys.getsizeof(u'123') => 52 Byte (원소가 1개 추가될 때마다 1 Byte씩 증가)\n",
    " * sys.getsizeof(u'가') => 76 Byte (27 Byte 증가)\n",
    " * sys.getsizeof(u'가나') => 78 Byte (원소가 1개 추가될 때마다 2 Byte씩 증가)\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Representation\n",
    "  * 모든 단어의 조합에 대해 다음과 같은 네트워크를 적용시킴\n",
    "  \n",
    "  $$ f(x_i, x_j ) =W \\phi(U_{left} e_i + U_{right} e_j)$$\n",
    "  \n",
    "  * Sentence representation과 classification을 구분짓기가 어려움\n",
    "    - BOW는 단순히 word vector의 합, 혹은 평균을 sentence representation으로 보고 이를 input으로 classify를 구성하였음\n",
    "      - 문장을 1개의 vector로 압축.\n",
    "    - CBOW는 one-hot representation을 보다 작은 차원의 embedding space로 mapping시킨 후에 embedding 결과를 classification의 input으로 사용\n",
    "      - 문장을 여러개의 vector로 압축. pre-trained embedding을 이용하면, classification과 sentence representation을 구분시킬 수 있으나,\n",
    "      - 만약 network에 녹여 동시에 학습을 시키려 한다면, 둘은 구분이 되지 않음\n",
    "    - 하나의 network로 두가지 task를 연속적으로 학습시켜야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RN_Classifier(nn.HybridBlock):\n",
    "    def __init__(self, SENTENCE_LENGTH, VOCABULARY, **kwargs):\n",
    "        super(RN_Classifier, self).__init__(**kwargs)\n",
    "        self.SENTENCE_LENGTH = SENTENCE_LENGTH\n",
    "        self.VOCABULARY = VOCABULARY\n",
    "        with self.name_scope():\n",
    "            self.g_fc1 = nn.Dense(256,activation='relu')\n",
    "            self.g_fc2 = nn.Dense(256,activation='relu')\n",
    "            self.g_fc3 = nn.Dense(256,activation='relu')\n",
    "            self.g_fc4 = nn.Dense(256,activation='relu')\n",
    "\n",
    "\n",
    "            self.fc1 = nn.Dense(128, activation = 'relu') # 256 * 128\n",
    "            self.fc2 = nn.Dense(2) # 128 * 2\n",
    "            # 1253632 param : 약 20MB\n",
    "    def hybrid_forward(self, F, x):\n",
    "        # (x_i, x_j)의 pair를 만들기\n",
    "        # 64 배치를 가정하면\n",
    "        print('x shape = {}'.format(x.shape))\n",
    "        x_i = x.expand_dims(1) # 64 * 1* 40 * 2000* : 0.02GB\n",
    "        print('x shape = {}'.format(x.shape))\n",
    "        x_i = F.repeat(x_i,repeats= self.SENTENCE_LENGTH, axis=1) # 64 * 40 * 40 * 2000: 1.52GB\n",
    "        print('x shape = {}'.format(x.shape))\n",
    "        x_j = x.expand_dims(2) # 64 * 40 * 1 * 2000\n",
    "        x_j = F.repeat(x_j,repeats= self.SENTENCE_LENGTH, axis=2) # 64 * 40 * 40 * 2000: 1.52GB\n",
    "        x_full = F.concat(x_i,x_j,dim=3) # 64 * 40 * 40 * 4000: 3.04GB\n",
    "        \n",
    "        # batch*sentence_length*sentence_length개의 batch를 가진 2*VOCABULARY input을 network에 feed\n",
    "        _x = x_full.reshape((-1, 2 * self.VOCABULARY))\n",
    "        _x = self.g_fc1(_x) # (64 * 40 * 40) * 256: .1GB 추가메모리는 안먹나?\n",
    "        _x = self.g_fc2(_x) # (64 * 40 * 40) * 256: .1GB (reuse)\n",
    "        _x = self.g_fc3(_x) # (64 * 40 * 40) * 256: .1GB (reuse)\n",
    "        _x = self.g_fc4(_x) # (64 * 40 * 40) * 256: .1GB (reuse)\n",
    "        \n",
    "        # sentence_length*sentence_length개의 결과값을 모두 합해서 sentence representation으로 나타냄\n",
    "        x_g = _x.reshape((-1, self.SENTENCE_LENGTH * self.SENTENCE_LENGTH,256)) # (64, 40*40, 256) : .1GB\n",
    "        sentence_rep = x_g.sum(1) # (64, 256): ignorable\n",
    "        \n",
    "        # 여기서부터는 classifier\n",
    "        clf = self.fc1(sentence_rep)\n",
    "        clf = self.fc2(clf)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (64, 40, 2000)\n"
     ]
    }
   ],
   "source": [
    "rn = RN_Classifier(MAX_FEATURES, 50)\n",
    "z = np.random.uniform(size = (64, 40, 2000))\n",
    "z1 = nd.array(z, ctx =context)\n",
    "rn(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 14219.05 MiB, increment: -0.01 MiB\n",
      "peak memory: 14219.08 MiB, increment: 0.02 MiB\n",
      "peak memory: 14219.08 MiB, increment: 0.00 MiB\n",
      "peak memory: 14219.08 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "rn = RN_Classifier(MAX_FEATURES, 50)\n",
    "\n",
    "rn.collect_params().initialize(mx.init.Xavier(), ctx = context)\n",
    "%memit(rn.g_fc1) # 4000 * 256 )\n",
    "%memit(rn.g_fc2) # 256 * 256\n",
    "%memit(rn.g_fc3) # 256 * 256\n",
    "%memit(rn.g_fc4) # 256 * 256\n",
    "loss = gluon.loss.SoftmaxCELoss()\n",
    "trainer = gluon.Trainer(rn.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f17c1a477a4b4da220d5d7416e7377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(train_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        with autograd.record():\n",
    "            print(_dat.shape)\n",
    "            _out = rn(_dat)\n",
    "            _los = nd.sum(loss(_out, _label)) # 배치의 크기만큼의 loss가 나옴\n",
    "            _los.backward()\n",
    "        trainer.step(_dat.shape[0])\n",
    "        n_obs += _dat.shape[0]\n",
    "        _total_los += nd.sum(_los).asnumpy()\n",
    "        # Epoch loss를 구하기 위해서 결과물을 계속 쌓음\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy()) # 두번째 컬럼의 확률이 예측 확률\n",
    "        label.extend(_label.asnumpy())\n",
    "    tr_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    tr_loss = _total_los/n_obs\n",
    "    \n",
    "    ### Evaluate training\n",
    "    valid_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(valid_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        _out = mlp(_dat)\n",
    "        _pred_score = nd.softmax(_out)\n",
    "        n_obs += _dat.shape[0]\n",
    "        _total_los += nd.sum(loss(_out, _label))\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy())\n",
    "        label.extend(_label.asnumpy())\n",
    "    va_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    va_loss = _total_los/n_obs\n",
    "    tqdm.write('Epoch {}: tr_loss = {}, tr_acc= {}, va_loss = {}, va_acc= {}'.format(epoch, tr_loss, tr_acc, va_loss, va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp(nd.array(va_x, ctx = context))\n",
    "# softmax를 적용하고\n",
    "# 두번째 열을 뽑아와서\n",
    "# nd.round 함수를 적용해서 0/1 예측값을 얻고\n",
    "# numpy array로 바꾸고\n",
    "# 첫번째 원소를 뽑아서 예측 label로 사용\n",
    "pred_mlp = [nd.round(val).asnumpy()[0] for val in nd.softmax(y_pred_mlp)[:, 1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.47%\n"
     ]
    }
   ],
   "source": [
    "accuracy_mlp = accuracy_score(va_y, pred_mlp)\n",
    "print('Accuracy: %.2f%%'%(accuracy_rf * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.array([1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(np.finfo(a[0]).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN without embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense1 = nn.Dense(64)\n",
    "            #self.dense2 = nn.Dense(32, activation = 'relu')\n",
    "            self.bn = nn.BatchNorm()\n",
    "            self.dense2 = nn.Dense(2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn(x)\n",
    "        x = nd.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fc1 = nn.Dense(256, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "batch_size = 64\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_no_embedding = MLP()\n",
    "mlp_no_embedding.collect_params().initialize(mx.init.Xavier(), ctx = context)\n",
    "loss = gluon.loss.SoftmaxCELoss()\n",
    "trainer = gluon.Trainer(mlp_no_embedding.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeferredInitializationError",
     "evalue": "Parameter mlp1_dense0_weight has not been initialized yet because initialization was deferred. Actual initialization happens during the first forward pass. Please pass one batch of data through the network before accessing Parameters. You can also avoid deferred initialization by specifying in_units, num_features, etc., for network layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeferredInitializationError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-51f0384c2cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mmlp_no_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmlp_no_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-51f0384c2cb4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mmlp_no_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmlp_no_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/gluon/parameter.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mNDArray\u001b[0m \u001b[0mon\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_and_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlist_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/venv/lib/python3.6/site-packages/mxnet/gluon/parameter.py\u001b[0m in \u001b[0;36m_check_and_get\u001b[0;34m(self, arr_list, ctx)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;34m\"Please pass one batch of data through the network before accessing Parameters. \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;34m\"You can also avoid deferred initialization by specifying in_units, \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \"num_features, etc., for network layers.\"%(self.name))\n\u001b[0m\u001b[1;32m    184\u001b[0m         raise RuntimeError(\n\u001b[1;32m    185\u001b[0m             \u001b[0;34m\"Parameter %s has not been initialized. Note that \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDeferredInitializationError\u001b[0m: Parameter mlp1_dense0_weight has not been initialized yet because initialization was deferred. Actual initialization happens during the first forward pass. Please pass one batch of data through the network before accessing Parameters. You can also avoid deferred initialization by specifying in_units, num_features, etc., for network layers."
     ]
    }
   ],
   "source": [
    "[mlp_no_embedding.collect_params()[x].data() for x in mlp_no_embedding.collect_params()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([Parameter mlp0_dense0_weight (shape=(64, 0), dtype=<class 'numpy.float32'>), Parameter mlp0_dense0_bias (shape=(64,), dtype=<class 'numpy.float32'>), Parameter mlp0_batchnorm0_gamma (shape=(0,), dtype=<class 'numpy.float32'>), Parameter mlp0_batchnorm0_beta (shape=(0,), dtype=<class 'numpy.float32'>), Parameter mlp0_batchnorm0_running_mean (shape=(0,), dtype=<class 'numpy.float32'>), Parameter mlp0_batchnorm0_running_var (shape=(0,), dtype=<class 'numpy.float32'>), Parameter mlp0_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>), Parameter mlp0_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_no_embedding.collect_params().values\n",
    "critic.collect_params()['critic1_dense0_weight'].data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b177abb5974ebf800aed89a569d597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: tr_loss = [0.00022346], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 1: tr_loss = [0.00019266], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 2: tr_loss = [0.00016774], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 3: tr_loss = [0.0001472], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 4: tr_loss = [0.00013019], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 5: tr_loss = [0.00011583], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 6: tr_loss = [0.00010363], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 7: tr_loss = [9.320182e-05], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 8: tr_loss = [8.417194e-05], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "Epoch 9: tr_loss = [7.6324715e-05], tr_acc= 1.0, va_loss = [0.03824826], va_acc= 0.9880514705882353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(n_epoch), desc = 'epoch'):\n",
    "    ## Training\n",
    "    train_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(train_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        with autograd.record():\n",
    "            _out = mlp_no_embedding(_dat)\n",
    "            _los = nd.sum(loss(_out, _label)) # 배치의 크기만큼의 loss가 나옴\n",
    "            _los.backward()\n",
    "        trainer.step(_dat.shape[0])\n",
    "        n_obs += _dat.shape[0]\n",
    "        #print(n_obs)\n",
    "        _total_los += nd.sum(_los).asnumpy()\n",
    "        # Epoch loss를 구하기 위해서 결과물을 계속 쌓음\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy()) # 두번째 컬럼의 확률이 예측 확률\n",
    "        label.extend(_label.asnumpy())\n",
    "    #print(pred)\n",
    "    #print([round(p) for p in pred]) # 기본이 float임\n",
    "    #print(label)\n",
    "    #print('**** ' + str(n_obs))\n",
    "    #print(label[:10])\n",
    "    #print(pred[:10])\n",
    "    #print([round(p) for p in pred][:10])\n",
    "    tr_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    tr_loss = _total_los/n_obs\n",
    "    \n",
    "    ### Evaluate training\n",
    "    valid_data.reset()\n",
    "    n_obs = 0\n",
    "    _total_los = 0\n",
    "    pred = []\n",
    "    label = []\n",
    "    for i, batch in enumerate(valid_data):\n",
    "        _dat = batch.data[0].as_in_context(context)\n",
    "        _label = batch.data[1].as_in_context(context)\n",
    "        _out = mlp(_dat)\n",
    "        _pred_score = nd.softmax(_out)\n",
    "        n_obs += _dat.shape[0]\n",
    "        _total_los += nd.sum(loss(_out, _label)).asnumpy()\n",
    "        pred.extend(nd.softmax(_out)[:,1].asnumpy())\n",
    "        label.extend(_label.asnumpy())\n",
    "    va_acc = accuracy_score(label, [round(p) for p in pred])\n",
    "    va_loss = _total_los/n_obs\n",
    "    tqdm.write('Epoch {}: tr_loss = {}, tr_acc= {}, va_loss = {}, va_acc= {}'.format(epoch, tr_loss, tr_acc, va_loss, va_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp_no_embedding = mlp_no_embedding(nd.array(va_x, ctx = context))\n",
    "# softmax를 적용하고\n",
    "# 두번째 열을 뽑아와서\n",
    "# nd.round 함수를 적용해서 0/1 예측값을 얻고\n",
    "# numpy array로 바꾸고\n",
    "# 첫번째 원소를 뽑아서 예측 label로 사용\n",
    "pred_mlp_no_embedding = [nd.round(val).asnumpy()[0] for val in nd.softmax(y_pred_mlp)[:, 1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.47%\n"
     ]
    }
   ],
   "source": [
    "accuracy_mlp_no_embedding = accuracy_score(va_y, pred_mlp_no_embedding)\n",
    "print('Accuracy: %.2f%%'%(accuracy_rf * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_txt = pd.DataFrame(np.array([origin_txt[idx] for idx in va_idx]), columns = ['txt'])\n",
    "pred_mlp_no_embedding_pd = pd.DataFrame(pred_mlp_no_embedding, columns  = ['pred_mlp_no_embedding'])\n",
    "label_pd = pd.DataFrame(va_y, columns = ['label'])\n",
    "result = pd.concat([va_txt, pred_mlp_no_embedding_pd, label_pd], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 3)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['pred_mlp_no_embedding'] != result['label']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[9.9985087e-01 9.9978369e-01 9.9972004e-01 9.9981624e-01 9.9978369e-01\n",
       " 4.8423914e-04 2.4478373e-01 9.1124475e-02 1.6645603e-03 4.0846759e-01\n",
       " 6.2343909e-04 3.0040858e-02 1.7331500e-03 1.4756978e-03 7.7909184e-01\n",
       " 2.6120720e-08 4.5528898e-01 1.1758124e-03 1.3725077e-02 1.5767918e-04\n",
       " 2.0312884e-06 2.2029674e-04 2.0878112e-04 9.5378027e-06 5.9853699e-02\n",
       " 1.5292705e-04 1.4807166e-04 3.5203213e-04 8.5062282e-05 8.4243221e-03\n",
       " 3.5203213e-04 8.5656675e-06]\n",
       "<NDArray 32 @gpu(0)>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pred_score[:, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
